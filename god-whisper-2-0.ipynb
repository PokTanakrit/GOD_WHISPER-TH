{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n  print('Not connected to a GPU')\nelse:\n  print(gpu_info)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:21:07.909662Z","iopub.execute_input":"2024-02-04T13:21:07.910020Z","iopub.status.idle":"2024-02-04T13:21:08.028117Z","shell.execute_reply.started":"2024-02-04T13:21:07.909991Z","shell.execute_reply":"2024-02-04T13:21:08.027138Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Sun Feb  4 13:21:07 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   38C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q datasets>=2.6.1\n!pip install -q git+https://github.com/huggingface/transformers\n!pip install -q librosa\n!pip install -q evaluate>=0.30\n!pip install -q jiwer\n!pip install -U -q accelerate\n!pip install -q torch==1.13.0\n!pip install -q torchaudio==2.0.0\n!pip install -q pythainlp\n!pip install -q sox\n# !pip install -q pydub\n# !pip install --upgrade -q tensorflow\n# !pip install --upgrade tensorflow-probability\n# !pip install accelerate\n# !pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:21:08.030069Z","iopub.execute_input":"2024-02-04T13:21:08.030391Z","iopub.status.idle":"2024-02-04T13:26:59.656011Z","shell.execute_reply.started":"2024-02-04T13:21:08.030336Z","shell.execute_reply":"2024-02-04T13:26:59.654845Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchdata 0.7.1 requires torch>=2, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'torchaudio' candidate (version 2.0.0 at https://files.pythonhosted.org/packages/40/23/4c8e2553b70eb46eb6d6989ee124707305e7c077a1bd16403f99f95f6a33/torchaudio-2.0.0-cp310-cp310-manylinux1_x86_64.whl (from https://pypi.org/simple/torchaudio/))\nReason for being yanked: Contains an incorrect torch dependency\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n#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\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:26:59.657532Z","iopub.execute_input":"2024-02-04T13:26:59.657849Z","iopub.status.idle":"2024-02-04T13:26:59.929017Z","shell.execute_reply.started":"2024-02-04T13:26:59.657817Z","shell.execute_reply":"2024-02-04T13:26:59.928077Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9a41b24136e4e2ab794c6da15b84cc8"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\ndataset = DatasetDict()\n\ndataset[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"th\", split=\"train[:2000]\", use_auth_token=True)\ndataset[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"th\", split=\"test[:2000]\", use_auth_token=True)\ndataset[\"predict\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"th\", split=\"test[2501:2520]\", use_auth_token=True)\ndataset = dataset.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:26:59.931837Z","iopub.execute_input":"2024-02-04T13:26:59.932485Z","iopub.status.idle":"2024-02-04T13:33:28.903633Z","shell.execute_reply.started":"2024-02-04T13:26:59.932448Z","shell.execute_reply":"2024-02-04T13:33:28.902676Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c570ad3f708488e9dd1fe7b1e4c29c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b7bc3dd981244af88c23eb65c916752"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/60.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0286b6fb6ed148f4a893e32de8a38ff0"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset common_voice/th to /root/.cache/huggingface/datasets/mozilla-foundation___common_voice/th/11.0.0/3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0327484d83194d5bb508e1e2ecd8fd23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64d90505b7764af582da5c8316ddc96b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/821M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e30942a4f0d149fd99f4c7c131af2423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65f8a0e7e74c48e7bb9540758665040a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/333M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bcc28b6b2274baa9734a3ba8c3a5c80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.15G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd844d9da5ea4590b3db80c8aa3848b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85801570d1774f96969377dc29849e08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/961M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8be886c779456687379d95813d20f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/824M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42c73622a8bf4ee9b7d5abe1dbf31a2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/847M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b25ca81e3214fac9c5a37b97242138b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/272M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeabff97a0ae4a7e8438c1ed01802088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd65170ff8cf46e7ada06ae50941259d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"760c2030689e4536a16304dbe2ebad42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.62M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb7b10ecc4a420f9750c604a84998bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.98M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3509c0d43ef1489e8c013a725bbdedb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed3ed0e8d314c1e9695128a704aa608"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/53.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"303f09200beb4dd7bd0f16b5a762ec6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.49M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a6acc030a9946bc8387a8bd72c2660a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c641a948be7e4394be4c0c37da62dfa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 8764it [00:00, 87610.84it/s]\u001b[A\nReading metadata...: 17820it [00:00, 89338.31it/s]\u001b[A\nReading metadata...: 31849it [00:00, 86790.56it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 10930it [00:00, 90435.85it/s][A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 10930it [00:00, 97368.14it/s] \u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating other split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 10189it [00:00, 101877.71it/s]\u001b[A\nReading metadata...: 20377it [00:00, 100461.65it/s]\u001b[A\nReading metadata...: 30767it [00:00, 102013.89it/s]\u001b[A\nReading metadata...: 40971it [00:00, 99710.59it/s] \u001b[A\nReading metadata...: 50950it [00:00, 97322.44it/s]\u001b[A\nReading metadata...: 60694it [00:00, 96608.96it/s]\u001b[A\nReading metadata...: 70362it [00:00, 94072.46it/s]\u001b[A\nReading metadata...: 79899it [00:00, 94469.02it/s]\u001b[A\nReading metadata...: 89356it [00:00, 90267.57it/s]\u001b[A\nReading metadata...: 98419it [00:01, 89511.59it/s]\u001b[A\nReading metadata...: 107517it [00:01, 89937.05it/s]\u001b[A\nReading metadata...: 116529it [00:01, 88004.29it/s]\u001b[A\nReading metadata...: 125553it [00:01, 88654.38it/s]\u001b[A\nReading metadata...: 134827it [00:01, 89849.36it/s]\u001b[A\nReading metadata...: 143825it [00:01, 89547.48it/s]\u001b[A\nReading metadata...: 152789it [00:01, 89034.87it/s]\u001b[A\nReading metadata...: 161699it [00:01, 88710.43it/s]\u001b[A\nReading metadata...: 170873it [00:01, 89606.03it/s]\u001b[A\nReading metadata...: 179838it [00:01, 87497.09it/s]\u001b[A\nReading metadata...: 195162it [00:02, 91443.61it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating invalidated split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"\nReading metadata...: 0it [00:00, ?it/s]\u001b[A\nReading metadata...: 8771it [00:00, 76017.56it/s]\u001b[A\n","output_type":"stream"},{"name":"stdout","text":"Dataset common_voice downloaded and prepared to /root/.cache/huggingface/datasets/mozilla-foundation___common_voice/th/11.0.0/3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"common_voice = dataset\nprint(common_voice)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:33:28.904871Z","iopub.execute_input":"2024-02-04T13:33:28.905477Z","iopub.status.idle":"2024-02-04T13:33:28.910906Z","shell.execute_reply.started":"2024-02-04T13:33:28.905439Z","shell.execute_reply":"2024-02-04T13:33:28.909789Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 2000\n    })\n    predict: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 19\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import WhisperFeatureExtractor\nfrom transformers import WhisperTokenizer\nfrom transformers import WhisperProcessor\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Thai\", task=\"transcribe\")\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Thai\", task=\"transcribe\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:33:28.912120Z","iopub.execute_input":"2024-02-04T13:33:28.912511Z","iopub.status.idle":"2024-02-04T13:33:37.636524Z","shell.execute_reply.started":"2024-02-04T13:33:28.912486Z","shell.execute_reply":"2024-02-04T13:33:37.635484Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d426f5bae64c48b107c7f392c6d09d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d3a9d31618e4a3781bbf6d36b66bae2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b325261f36704a78b7fa99757198fa5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5b584a06b8402fae7b073d82bdcbe9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e812be94f874fc6a33df475d6a7121f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ea12bdd4a3c48a0b031ea153b4976ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6779a0db5ba47169bfb2271fd253365"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a11124548444a4ab65cb007ed3f47af"}},"metadata":{}}]},{"cell_type":"code","source":"audio_data = common_voice[\"train\"][0][\"audio\"]\nsentence = common_voice[\"train\"][0][\"sentence\"]\nprint(\"Audio Data:\", audio_data)\nprint(\"Sentence:\", sentence)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:33:37.637978Z","iopub.execute_input":"2024-02-04T13:33:37.638575Z","iopub.status.idle":"2024-02-04T13:33:38.597655Z","shell.execute_reply.started":"2024-02-04T13:33:37.638528Z","shell.execute_reply":"2024-02-04T13:33:38.596639Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Audio Data: {'path': '/root/.cache/huggingface/datasets/downloads/extracted/ba89929447def0ce3ee0f7f12a0e84610a6b5603c1b2986d9f3a55c00075dcad/th_train_0/common_voice_th_31867109.mp3', 'array': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32), 'sampling_rate': 48000}\nSentence: ขายที่นาพิพาทให้จำเลยที่สอง และที่สาม\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nfrom pythainlp.tokenize import word_tokenize\n\nchars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\'\\~\\’\\_]'\n\n# def th_tokenize(batch):\n# #     print(f\"Before tokenization: {batch['sentence']}\")\n#     batch[\"sentence\"] = \" \".join(word_tokenize(batch[\"sentence\"], engine=\"newmm\"))\n# #     print(f\"After tokenization: {batch['sentence']}\")\n#     return batch\n\ndef remove_special_characters(batch):\n#     print(f\"Before removal: {batch['sentence']}\")\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n#     print(f\"After removal: {batch['sentence']}\")\n    return batch\n\n# common_voice= common_voice.map(th_tokenize)\ncommon_voice= common_voice.map(remove_special_characters)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:33:38.599273Z","iopub.execute_input":"2024-02-04T13:33:38.599867Z","iopub.status.idle":"2024-02-04T13:33:43.273958Z","shell.execute_reply.started":"2024-02-04T13:33:38.599826Z","shell.execute_reply":"2024-02-04T13:33:43.273055Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d210d66ea842c2bce8bb8a088a7d91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61ec6ceaf7a54aabab3c34c453de41c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a2735f8d7164269bccf214827f52389"}},"metadata":{}}]},{"cell_type":"code","source":"import re\n\n# Function to check if a word is English\ndef is_english_word(word):\n    # Regular expression to match English characters\n    english_pattern = re.compile(r\"[A-Za-z]+\")\n    return bool(english_pattern.match(word))\n\n# Iterate through the train and test splits\nfor split_name in [\"train\", \"test\" , \"predict\"]:\n    print(f\"Checking {split_name} split:\")\n    for example in common_voice[split_name]:\n        sentence = example[\"sentence\"]\n        # Split the sentence into words\n        words = sentence.split()\n        # Check each word\n        english_words = [word for word in words if is_english_word(word)]\n        # If there are English words, print the sentence\n        if english_words:\n            print(sentence)\n            print(\"English words:\", english_words)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:33:43.275192Z","iopub.execute_input":"2024-02-04T13:33:43.275561Z","iopub.status.idle":"2024-02-04T13:34:08.019910Z","shell.execute_reply.started":"2024-02-04T13:33:43.275526Z","shell.execute_reply":"2024-02-04T13:34:08.018811Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Checking train split:\nChecking test split:\nเนื่องจากการขาดโปรแกรมความผิดพลาด johanna จึงตัดสินใจขายการหาประโยชน์ในตลาดมืด \nEnglish words: ['johanna']\nฉันสงสัยว่า csection จะได้รับความนิยมมากกว่าการเกิดตามธรรมชาติในวันหนึ่ง \nEnglish words: ['csection']\nChecking predict split:\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\n\n# Filter sentences containing English characters\nenglish_sentences_train = [item['sentence'] for item in common_voice['train'] if re.search('[a-zA-Z]', item['sentence'])]\nenglish_sentences_test = [item['sentence'] for item in common_voice['test'] if re.search('[a-zA-Z]', item['sentence'])]\nenglish_sentences_predict = [item['sentence'] for item in common_voice['predict'] if re.search('[a-zA-Z]', item['sentence'])]\n\nprint(\"English sentences in train split:\", english_sentences_train)\nprint(\"English sentences in test split:\", english_sentences_test)\nprint(\"English sentences in predict split:\", english_sentences_predict)\n\n# Function to check for A-Z or a-z in the \"sentence\" column and drop those rows\ndef remove_rows_with_english_chars(batch):\n    return None if re.search('[A-Za-z]', batch[\"sentence\"]) else batch\n\n# Remove rows with A-Z or a-z characters only from the \"sentence\" column in common_voice_train and common_voice_test\ncommon_voice_train_filtered = common_voice['train'].filter(remove_rows_with_english_chars)\ncommon_voice_test_filtered = common_voice['test'].filter(remove_rows_with_english_chars)\ncommon_voice_predict_filtered = common_voice['predict'].filter(remove_rows_with_english_chars)\n\n# Update the original dataset with the filtered datasets\ncommon_voice['train'] = common_voice_train_filtered\ncommon_voice['test'] = common_voice_test_filtered\ncommon_voice['predict'] = common_voice_predict_filtered\n\n# Print the updated dataset\nprint(common_voice)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:34:08.023896Z","iopub.execute_input":"2024-02-04T13:34:08.024205Z","iopub.status.idle":"2024-02-04T13:34:56.214065Z","shell.execute_reply.started":"2024-02-04T13:34:08.024178Z","shell.execute_reply":"2024-02-04T13:34:56.213012Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"English sentences in train split: []\nEnglish sentences in test split: ['เนื่องจากการขาดโปรแกรมความผิดพลาด johanna จึงตัดสินใจขายการหาประโยชน์ในตลาดมืด ', 'ฉันสงสัยว่า csection จะได้รับความนิยมมากกว่าการเกิดตามธรรมชาติในวันหนึ่ง ']\nEnglish sentences in predict split: []\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff8ba77dd2fc44c8bfe2d360baab617a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0486ff33ba314b3fab6251ba33547c4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33d6cecb8f724cdfad7e9199ed5ec5b6"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 1998\n    })\n    predict: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 19\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\n\n# Function to check if a word is Thai\ndef is_thai_word(word):\n    # Regular expression to match Thai characters\n    thai_pattern = re.compile(r\"[\\u0E00-\\u0E7F]+\")\n    return bool(thai_pattern.match(word))\n\nfor split_name in [\"train\", \"test\" , \"predict\"]:\n    print(f\"Checking {split_name} split:\")\n    for example in common_voice[split_name]:\n        sentence = example[\"sentence\"]\n        words = sentence.split()\n        non_thai_words = [word for word in words if not is_thai_word(word)]\n        if non_thai_words:\n            print(\"Non-Thai words:\", non_thai_words)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:34:56.215348Z","iopub.execute_input":"2024-02-04T13:34:56.215689Z","iopub.status.idle":"2024-02-04T13:35:20.375574Z","shell.execute_reply.started":"2024-02-04T13:34:56.215662Z","shell.execute_reply":"2024-02-04T13:35:20.374490Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Checking train split:\nChecking test split:\nChecking predict split:\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Audio\n\ncommon_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:35:20.377163Z","iopub.execute_input":"2024-02-04T13:35:20.377550Z","iopub.status.idle":"2024-02-04T13:35:20.392980Z","shell.execute_reply.started":"2024-02-04T13:35:20.377516Z","shell.execute_reply":"2024-02-04T13:35:20.392024Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nnp.object = object","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:35:20.394215Z","iopub.execute_input":"2024-02-04T13:35:20.394847Z","iopub.status.idle":"2024-02-04T13:35:20.398995Z","shell.execute_reply.started":"2024-02-04T13:35:20.394815Z","shell.execute_reply":"2024-02-04T13:35:20.397980Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def prepare_dataset(batch):\n    # load and resample audio data from 48 to 16kHz\n    audio = batch[\"audio\"]\n#     print( \" audio =\", audio)\n#     print(\"---------------------------------------------------------------------\")\n    # compute log-Mel input features from input audio array\n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n#     print(\" input_features =\", batch[\"input_features\"])\n#     print(\"---------------------------------------------------------------------\")\n    # encode target text to label ids\n    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n#     print(\"labels =\", batch[\"input_features\"])\n#     print(\"---------------------------------------------------------------------\")\n    return batch\n\n# Add input_features and labels to the dataset\ncommon_voice[\"train\"] = common_voice[\"train\"].map(prepare_dataset)\ncommon_voice[\"test\"] = common_voice[\"test\"].map(prepare_dataset)  \ncommon_voice[\"predict\"] = common_voice[\"predict\"].map(prepare_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:35:20.400193Z","iopub.execute_input":"2024-02-04T13:35:20.401009Z","iopub.status.idle":"2024-02-04T13:44:25.656200Z","shell.execute_reply.started":"2024-02-04T13:35:20.400978Z","shell.execute_reply":"2024-02-04T13:44:25.655237Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2850b0ce73b4c06b1682ee4835b7b55"}},"metadata":{}},{"name":"stderr","text":"2024-02-04 13:35:22.572862: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-04 13:35:22.572971: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-04 13:35:22.726863: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1998 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22ca1d457e1844759f88c02a08b955aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d7e0ff82d9f472fb80aafb5b52b319b"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch\n    \ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:25.657646Z","iopub.execute_input":"2024-02-04T13:44:25.658276Z","iopub.status.idle":"2024-02-04T13:44:25.668986Z","shell.execute_reply.started":"2024-02-04T13:44:25.658248Z","shell.execute_reply":"2024-02-04T13:44:25.668071Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # replace -100 with the pad_token_id\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    # we do not want to group tokens when computing the metrics\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:25.670333Z","iopub.execute_input":"2024-02-04T13:44:25.670752Z","iopub.status.idle":"2024-02-04T13:44:29.475821Z","shell.execute_reply.started":"2024-02-04T13:44:25.670714Z","shell.execute_reply":"2024-02-04T13:44:29.474801Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c859bb14f5fc436e9bdf4f0240cd3e7b"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\n\n# model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\", load_in_8bit=True, device_map=\"auto\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:29.477150Z","iopub.execute_input":"2024-02-04T13:44:29.477762Z","iopub.status.idle":"2024-02-04T13:44:35.657575Z","shell.execute_reply.started":"2024-02-04T13:44:29.477734Z","shell.execute_reply":"2024-02-04T13:44:35.656823Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824285a3435e43678b6df6657936faa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d2f87ebc9154da797b1b0666891a233"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f76f8a47e894454adaf79a851ce2ea2"}},"metadata":{}}]},{"cell_type":"code","source":"model.config.forced_decoder_ids = processor.tokenizer.get_decoder_prompt_ids(\nlanguage=\"Thai\", task=\"transcribe\"\n)\nmodel.config.suppress_tokens = []\nmodel.generation_config.forced_decoder_ids = processor.tokenizer.get_decoder_prompt_ids(\nlanguage=\"Th\", task=\"transcribe\"\n)\nmodel.generation_config.suppress_tokens = []","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:35.658967Z","iopub.execute_input":"2024-02-04T13:44:35.659398Z","iopub.status.idle":"2024-02-04T13:44:35.665508Z","shell.execute_reply.started":"2024-02-04T13:44:35.659341Z","shell.execute_reply":"2024-02-04T13:44:35.664434Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# from peft import prepare_model_for_int8_training\n\n# model = prepare_model_for_int8_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:35.666775Z","iopub.execute_input":"2024-02-04T13:44:35.667675Z","iopub.status.idle":"2024-02-04T13:44:35.674859Z","shell.execute_reply.started":"2024-02-04T13:44:35.667642Z","shell.execute_reply":"2024-02-04T13:44:35.673882Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# def make_inputs_require_grad(module, input, output):\n#     output.requires_grad_(True)\n\n# model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:35.676046Z","iopub.execute_input":"2024-02-04T13:44:35.676816Z","iopub.status.idle":"2024-02-04T13:44:35.683736Z","shell.execute_reply.started":"2024-02-04T13:44:35.676783Z","shell.execute_reply":"2024-02-04T13:44:35.682913Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n\n# lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n\n# model = get_peft_model(model, lora_config)\n# model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:35.684958Z","iopub.execute_input":"2024-02-04T13:44:35.685240Z","iopub.status.idle":"2024-02-04T13:44:35.693214Z","shell.execute_reply.started":"2024-02-04T13:44:35.685215Z","shell.execute_reply":"2024-02-04T13:44:35.692322Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Define the new output directory path\nnew_output_dir = \"/kaggle/working/Whisper-TH\"\n\nfrom transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=new_output_dir,  # ไดเร็กทอรีที่จะบันทึกเช็กพอยท์และบันทึกล็อก\n    per_device_train_batch_size=4,  # ขนาดแบทช์ต่ออุปกรณ์รับส่งข้อมูลต่อการฝึก\n    gradient_accumulation_steps=2,  # จำนวนขั้นตอนการสะสมเกรเดียนต่อรอบ\n    learning_rate=1e-5,  # อัตราการเรียนรู้สำหรับตัวปรับปรุง\n    warmup_steps=100,  # จำนวนขั้นตอนการเตรียมความพร้อมสำหรับตัวปรับปรุงอัตราการเรียนรู้\n    max_steps=300,  # จำนวนขั้นตอนการฝึกสูงสุด\n    gradient_checkpointing=True,  # ใช้เช็กพอยท์เกรเดียนเพื่อประหยัดหน่วยความจำหรือไม่\n    fp16=True,  # ใช้การฝึกด้วยการสัมผัสส่วนที่มีความแม่นยำสูงแบบ FP16 หรือไม่\n    evaluation_strategy=\"steps\",  # กลยุทธ์การประเมินในระหว่างการฝึก\n    per_device_eval_batch_size=4,  # ขนาดแบทช์ต่ออุปกรณ์รับส่งข้อมูลต่อการประเมิน\n    predict_with_generate=True,  # ใช้การสร้างข้อมูลในระหว่างการประเมินหรือไม่\n    generation_max_length=225,  # ความยาวสูงสุดของชุดตัวอย่างที่สร้างขึ้นในระหว่างการประเมิน\n    eval_steps=300,  # จำนวนขั้นตอนระหว่างการประเมิน\n    report_to=[\"tensorboard\"],  # รายการของการรวบรวมสถิติการฝึกไปยังแพลตฟอร์ม\n    metric_for_best_model=\"wer\",  # เมตริกที่ใช้ในการระบุโมเดลที่ดีที่สุด\n    greater_is_better=False,  # ค่าที่สูงขึ้นของเมตริกดีกว่าหรือไม่\n    logging_strategy=\"steps\",  # กลยุทธ์การบันทึกล็อกสถิติการฝึก\n    logging_steps=100,  # บันทึกสถิติการฝึกทุก `logging_steps`\n#     push_to_hub=True,\n)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:27.730038Z","iopub.execute_input":"2024-02-04T14:59:27.730728Z","iopub.status.idle":"2024-02-04T14:59:27.739054Z","shell.execute_reply.started":"2024-02-04T14:59:27.730691Z","shell.execute_reply":"2024-02-04T14:59:27.738154Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=common_voice[\"train\"],\n    eval_dataset=common_voice[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:31.699278Z","iopub.execute_input":"2024-02-04T14:59:31.699605Z","iopub.status.idle":"2024-02-04T14:59:31.719912Z","shell.execute_reply.started":"2024-02-04T14:59:31.699579Z","shell.execute_reply":"2024-02-04T14:59:31.719121Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:34.099171Z","iopub.execute_input":"2024-02-04T14:59:34.100120Z","iopub.status.idle":"2024-02-04T17:15:42.467349Z","shell.execute_reply.started":"2024-02-04T14:59:34.100074Z","shell.execute_reply":"2024-02-04T17:15:42.466374Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 2:15:43, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>300</td>\n      <td>0.120800</td>\n      <td>0.304398</td>\n      <td>88.752960</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=300, training_loss=0.2032380771636963, metrics={'train_runtime': 8167.2224, 'train_samples_per_second': 0.588, 'train_steps_per_second': 0.037, 'total_flos': 1.385209921536e+18, 'train_loss': 0.2032380771636963, 'epoch': 2.4})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Step\tTraining Loss\tValidation Loss\tWer\n# 100\tNo log\t        0.547743\t99.336912\n# 200\tNo log\t0.342716\t93.369119\n# 300\tNo log\t0.312836\t89.801074\n# 400\tNo log\t0.297427\t94.411115\n# 500\t0.552800\t0.288857\t91.095674\n","metadata":{}},{"cell_type":"code","source":"# training_args = Seq2SeqTrainingArguments(\n#     output_dir=new_output_dir,  \n#     per_device_train_batch_size=4,\n#     gradient_accumulation_steps=1,\n#     learning_rate=1e-5,\n#     warmup_steps=50,  # Reduced warmup steps\n#     max_steps=500,\n#     gradient_checkpointing=True,\n#     fp16=True,\n#     evaluation_strategy=\"steps\",\n#     per_device_eval_batch_size=4,\n#     predict_with_generate=True,\n#     generation_max_length=225,\n#     eval_steps=100,\n#     report_to=[\"tensorboard\"],\n#     metric_for_best_model=\"wer\",\n#     greater_is_better=False,\n# )","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.518984Z","iopub.status.idle":"2024-02-04T13:44:41.519417Z","shell.execute_reply.started":"2024-02-04T13:44:41.519197Z","shell.execute_reply":"2024-02-04T13:44:41.519214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/model2\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T14:59:05.111594Z","iopub.status.idle":"2024-02-04T14:59:05.112081Z","shell.execute_reply.started":"2024-02-04T14:59:05.111837Z","shell.execute_reply":"2024-02-04T14:59:05.111859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.522644Z","iopub.status.idle":"2024-02-04T13:44:41.523055Z","shell.execute_reply.started":"2024-02-04T13:44:41.522841Z","shell.execute_reply":"2024-02-04T13:44:41.522857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Train the model\n# trainer.train()\n# # Save the trained model\n# # trainer.save_model(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.524322Z","iopub.status.idle":"2024-02-04T13:44:41.524760Z","shell.execute_reply.started":"2024-02-04T13:44:41.524546Z","shell.execute_reply":"2024-02-04T13:44:41.524564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"learning_rate=1e-5,","metadata":{}},{"cell_type":"code","source":"# # Train the model\n# trainer.train()\n# # Save the trained model\n# # trainer.save_model(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.526101Z","iopub.status.idle":"2024-02-04T13:44:41.526530Z","shell.execute_reply.started":"2024-02-04T13:44:41.526305Z","shell.execute_reply":"2024-02-04T13:44:41.526321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"learning_rate=0.05,","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Before allocating new memory\nprint(torch.cuda.memory_allocated())  # Current memory allocated\nprint(torch.cuda.memory_cached())     # Current memory cached\n\n# Free the cached memory\ntorch.cuda.empty_cache()\n\n# After freeing the memory\nprint(torch.cuda.memory_allocated())  # Current memory allocated\nprint(torch.cuda.memory_cached())     # Current memory cached\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.529155Z","iopub.status.idle":"2024-02-04T13:44:41.529937Z","shell.execute_reply.started":"2024-02-04T13:44:41.529703Z","shell.execute_reply":"2024-02-04T13:44:41.529722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Train the model\n# trainer.train()\n# # Save the trained model\n# # trainer.save_model(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.530853Z","iopub.status.idle":"2024-02-04T13:44:41.531296Z","shell.execute_reply.started":"2024-02-04T13:44:41.531058Z","shell.execute_reply":"2024-02-04T13:44:41.531075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n\n# # Define the directory to be deleted\n# directory_to_delete = \"/kaggle/working/Whisper-TH\"\n\n# # Attempt to delete the directory\n# try:\n#     shutil.rmtree(directory_to_delete)\n#     print(f\"Directory '{directory_to_delete}' deleted successfully.\")\n# except Exception as e:\n#     print(f\"Error deleting directory '{directory_to_delete}': {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.533155Z","iopub.status.idle":"2024-02-04T13:44:41.533746Z","shell.execute_reply.started":"2024-02-04T13:44:41.533518Z","shell.execute_reply":"2024-02-04T13:44:41.533537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_voice[\"predict\"]","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:15:42.817637Z","iopub.execute_input":"2024-02-04T17:15:42.818013Z","iopub.status.idle":"2024-02-04T17:15:42.824617Z","shell.execute_reply.started":"2024-02-04T17:15:42.817979Z","shell.execute_reply":"2024-02-04T17:15:42.823676Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['audio', 'sentence', 'input_features', 'labels'],\n    num_rows: 19\n})"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n# Empty the cache to free up memory\ntorch.cuda.empty_cache()\n\n# Set the fraction of memory allocated to PyTorch\n# This example sets it to 0.6, meaning PyTorch can use up to 60% of the GPU memory\ntorch.cuda.set_per_process_memory_fraction(0.6)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.537818Z","iopub.status.idle":"2024-02-04T13:44:41.538620Z","shell.execute_reply.started":"2024-02-04T13:44:41.538387Z","shell.execute_reply":"2024-02-04T13:44:41.538408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef generate_text(examples):\n    # Get the input features\n    input_features = torch.tensor(examples[\"input_features\"])  # Convert to tensor\n    padding_length = 3000 - input_features.shape[-1]\n    padded_input_features = torch.nn.functional.pad(input_features, (0, padding_length), \"constant\", 0)\n    \n    # Pass the input features through the model's encoder\n    with torch.no_grad():\n        encoder_outputs = model.get_encoder()(padded_input_features.to(model.device))\n    \n    # Generate text using the encoder outputs\n    outputs = model.generate(encoder_outputs=encoder_outputs, return_dict_in_generate=True)  # Ensure input is on the same device as the model\n    generated_texts = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n    examples[\"generated_text\"] = generated_texts\n    \n    return examples\n\ndef print_predictions(predictions):\n    # Print original sentences and generated texts together\n    for original, generated in zip(predictions[\"sentence\"], predictions[\"generated_text\"]):\n        print(\"Original Sentence:\", original)\n        print(\"Generated Text:\", generated)\n        print()  # Add an empty line for better readability\n\ntest_subset = common_voice[\"test\"].select(list(range(10)))\n\n# Generate predictions on the test subset dataset\npredictions = test_subset.map(generate_text, batched=True)\n\n# Print original sentences and generated texts together\nprint_predictions(predictions)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:15:54.511830Z","iopub.execute_input":"2024-02-04T17:15:54.512544Z","iopub.status.idle":"2024-02-04T17:15:59.549146Z","shell.execute_reply.started":"2024-02-04T17:15:54.512511Z","shell.execute_reply":"2024-02-04T17:15:59.548006Z"},"trusted":true},"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c04f49c72ab421ab5ae186c23064339"}},"metadata":{}},{"name":"stdout","text":"Original Sentence: ใครเป็นผู้รับ \nGenerated Text: ใครเป็นผู้รับ \n\nOriginal Sentence: รู้ได้ไงว่าเขาไม่หนุก \nGenerated Text: รู้ได้ไงว่าเขาไม่นุก \n\nOriginal Sentence: การที่จะทำเค้กแต่งงานชั้นเลิศคุณจะต้องใช้น้ำตาลไอซิ่ง \nGenerated Text: การที่จะทำเคคแต่งงาน ฉันเลิกคุณจะต้องใช้น้ำตาลไอซิง \n\nOriginal Sentence: เขาหยุด แล้วคว้ามือออกมา เฉียดตัวฉันไปเส้นยาแเงผ่าแปด \nGenerated Text: เขาหยุดและความเงือกมาเชียดตัวฉันไปเส้นยะแดงภาแปด \n\nOriginal Sentence: ผู้ชายคือช้างเท้าหน้า แต่ผู้หญิงคือควาญช้าง \nGenerated Text: ผู้ชายคือชางเทาหน้าแต่ผู้หญิงคือขวานชาง แอ้แห้ \n\nOriginal Sentence: มันช่วยให้ฉันจำได้ \nGenerated Text: มันช่วยให้ฉันจำได้ \n\nOriginal Sentence: กูเคยบอกมึงหลายทีแล้ว \nGenerated Text: ผมเคยบอกเมื่อมันหลายที่แล้ว \n\nOriginal Sentence: เขาเห็นตัวตลกเดินผ่านที่หน้าต่าง \nGenerated Text: เขาเห็นตัวตลกเดินผ่านที่หน้าต่าง \n\nOriginal Sentence: ลองชิมสลัดนี้ดูสิ \nGenerated Text: ลองชิมสลัดนี้ดูสิ \n\nOriginal Sentence: ระบบของเราจะเข้ารหัสและแฮรหัสผ่านของคุณเพื่อความปลอดภัย \nGenerated Text: ระบบของเราจะเข้าระหัดและแหร่ระหัดผ่านของคุณเพื่อความปรัฐฐ์ \n\n","output_type":"stream"}]},{"cell_type":"code","source":"# บันทึกโมเดลไปยังไดเรกทอรี\nsave_directory = \"/kaggle/working/model\"\nmodel.save_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T17:20:21.822135Z","iopub.execute_input":"2024-02-04T17:20:21.822533Z","iopub.status.idle":"2024-02-04T17:20:23.878840Z","shell.execute_reply.started":"2024-02-04T17:20:21.822501Z","shell.execute_reply":"2024-02-04T17:20:23.877913Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# dataset[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"th\", split=\"test[2501:3501]\", use_auth_token=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.543978Z","iopub.status.idle":"2024-02-04T13:44:41.544841Z","shell.execute_reply.started":"2024-02-04T13:44:41.544596Z","shell.execute_reply":"2024-02-04T13:44:41.544616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datatest = dataset[\"test\"]\n# print(datatest)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.546183Z","iopub.status.idle":"2024-02-04T13:44:41.546619Z","shell.execute_reply.started":"2024-02-04T13:44:41.546401Z","shell.execute_reply":"2024-02-04T13:44:41.546418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datatest = datatest.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n# print(datatest)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.547741Z","iopub.status.idle":"2024-02-04T13:44:41.548138Z","shell.execute_reply.started":"2024-02-04T13:44:41.547937Z","shell.execute_reply":"2024-02-04T13:44:41.547953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"remove_special_characters\")\n# datatest= datatest.map(remove_special_characters)\n# print(\"remove_rows_with_english_chars\")\n# datatest = datatest.filter(remove_rows_with_english_chars)\n# print(\"re_sampling_rate\")\n# datatest = datatest.cast_column(\"audio\", Audio(sampling_rate=16000))\n# print(\"prepare_dataset\")\n# datatest = datatest.map(prepare_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.549308Z","iopub.status.idle":"2024-02-04T13:44:41.549724Z","shell.execute_reply.started":"2024-02-04T13:44:41.549521Z","shell.execute_reply":"2024-02-04T13:44:41.549537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(datatest)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.552761Z","iopub.status.idle":"2024-02-04T13:44:41.553605Z","shell.execute_reply.started":"2024-02-04T13:44:41.553380Z","shell.execute_reply":"2024-02-04T13:44:41.553399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_subset = datatest\n\n# # Generate predictions on the test subset dataset\n# predictions = test_subset.map(generate_text, batched=True)\n\n# # Print original sentences and generated texts together\n# for original, generated in zip(predictions[\"sentence\"], predictions[\"generated_text\"]):\n#     print(\"Original Sentence:\", original)\n#     print(\"Generated Text:\", generated)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.554966Z","iopub.status.idle":"2024-02-04T13:44:41.555442Z","shell.execute_reply.started":"2024-02-04T13:44:41.555191Z","shell.execute_reply":"2024-02-04T13:44:41.555208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git config --global user.email \"s6404062636056@email.kmutnb.ac.th\"\n# !git config --global user.name \"PokTanakrit\"\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:41.557284Z","iopub.status.idle":"2024-02-04T13:44:41.557717Z","shell.execute_reply.started":"2024-02-04T13:44:41.557511Z","shell.execute_reply":"2024-02-04T13:44:41.557527Z"},"trusted":true},"execution_count":null,"outputs":[]}]}